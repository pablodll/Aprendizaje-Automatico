{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 4 (Entrenamiento de redes neuronales)\n",
    "### Aprendizaje automático y big data\n",
    "##### _Alberto García Doménech - Pablo Daurell Marina_    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta práctica será implementar los calculos y algoritmos necesarios para entrenar a una red neuronal.\n",
    "Usaremos una red neuronal para intentar clasificar 5000 imagenes de números del 0 al 9 escritos a mano. Cada imagen (de 20x20 pixeles) vendrá representada por un array de 400 componentes.   \n",
    "   \n",
    "Para ello haremos uso de una red neuronal de 3 capas, con una capa de entrada de 5000 neuronas, una capa oculta de 25 neuronas y una capa de salida de 10 neuronas (una por cada digito)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se trata de un problema de multiclasificación codificaremos el array _y_ en codificación one-hot (array de 9 elementos con un 1 en la clase correspondiente y 0s en el resto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos: 5000 ejemplos de entrenamiento (imagenes de 20x20 pixeles)\n",
    "data = loadmat('ex4data1.mat')\n",
    "\n",
    "X = data['X'] # Shape: (5000, 400)\n",
    "y = data['y'].ravel() # Shape: (5000,)\n",
    "\n",
    "# Establecemos la estructura de la red neuronal\n",
    "num_entradas = np.shape(X)[1]\n",
    "num_etiquetas = 10 # Del 0 al 9\n",
    "num_ocultas = 25\n",
    "\n",
    "# Adaptamos 'y' para usarlo en la red neuronal\n",
    "y = (y - 1)\n",
    "y_onehot = np.zeros((len(y), num_etiquetas)) # Shape: (5000, 10)\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y_onehot[i][y[i]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos una pequeña muestra de las imagenes del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionamos 10 numeros al azar y los mostramos\n",
    "from displayData import displayData\n",
    "sample = np.random.choice(X.shape[0], 100)\n",
    "img = displayData(X[sample,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como función de activación para las neuronas usaremos la funcion sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el algoritmo de propagación hacia delante que aplica los pesos correspondientes a cada capa de la red y nos permite clasificar los ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, theta1, theta2):\n",
    "    m = X.shape[0]\n",
    "    a1 = np.hstack([np.ones([m, 1]), X]) \n",
    "    z2 = np.dot(a1, theta1.T)\n",
    "    a2 = np.hstack([np.ones([m, 1]), sigmoid(z2)])\n",
    "    z3 = np.dot(a2, theta2.T)\n",
    "    h = sigmoid(z3)\n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de retropropagación (Back-propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coste regularizado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar a implementar el algoritmo de retropropagación.   \n",
    "\n",
    "- Primero establecemos como calcular el error (coste) de la red una vez aplicado el algoritmo de propagación hacia delante.\n",
    "- Ademas le añadimos el coeficiente de regularización para evitar el sobreaprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "    '''Funcion de back-propagation para red neuronal de 3 capas'''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Desplegamos los paramas_rn en la matrices Theta\n",
    "    theta1 = np.reshape(params_rn[:num_ocultas * (num_entradas + 1)],\n",
    "                       (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(params_rn[num_ocultas * (num_entradas + 1):],\n",
    "                       (num_etiquetas, (num_ocultas + 1)))\n",
    "    \n",
    "    # Aplicamos forward-propagation para calcular la salidas de cada capa\n",
    "    a1, z2, a2, z3, h = forward_propagation(X, theta1, theta2)\n",
    "    \n",
    "    # Calculo del coste\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        a = np.dot(-y[i,:], np.log(h[i,:]))\n",
    "        b = np.dot(1 - y[i,:], np.log(1 - h[i,:]))\n",
    "        cost += np.sum(a - b)\n",
    "        \n",
    "    cost = cost/m\n",
    "    cost += reg/(2*m) * (np.sum(theta1[:, 1:]**2) + np.sum(theta2[:, 1:]**2))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cargamos los parametros de la red neuronal ya entrenada para comprobar si hemos implementado bien el cálculo del coste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = loadmat('ex4weights.mat')\n",
    "\n",
    "theta1 = weights['Theta1'] # Shape: (25, 401)\n",
    "theta2 =weights['Theta2'] # Shape: (10, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste sin regularización:  0.2876291651613187\n",
      "Coste con regularización:  0.3837698590909234\n"
     ]
    }
   ],
   "source": [
    "thetaVec = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "print('Coste sin regularización: ', backprop(thetaVec, num_entradas, num_ocultas, num_etiquetas, X, y_onehot, 0))\n",
    "print('Coste con regularización: ',backprop(thetaVec, num_entradas, num_ocultas, num_etiquetas, X, y_onehot, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimos con la implementación de la retropropagación.  \n",
    "\n",
    "- Ahora vamos a añadir el cálculo de los vectores de gradientes, para ello implementaremos el algoritmo de retropropagación propiamente dicho para calcular las derivadas parciales del error en cada capa y poder computar el gradiente adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Antes de nada definimos la derivada de la función sigmoide (usada en el calculo del gradiente) y un método para inicializar las matrices de pesos con valores aleatorios, distintos pero cercanos a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesosAleatorios(L_in, L_out):\n",
    "    ini_epsilon = 0.12\n",
    "    theta = np.random.rand(L_out, 1 + L_in) * (2*ini_epsilon) - ini_epsilon \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementación del gradiente y la retropropagación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "    '''Funcion de back-propagation para red neuronal de 3 capas'''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Desplegamos los paramas_rn en la matrices Theta\n",
    "    theta1 = np.reshape(params_rn[:num_ocultas * (num_entradas + 1)],\n",
    "                       (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(params_rn[num_ocultas * (num_entradas + 1):],\n",
    "                       (num_etiquetas, (num_ocultas + 1)))\n",
    "    \n",
    "    # Aplicamos forward-propagation para calcular la salidas de cada capa\n",
    "    a1, z2, a2, z3, h = forward_propagation(X, theta1, theta2)\n",
    "    \n",
    "    # Calculo del coste\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        a = np.dot(-y[i,:], np.log(h[i,:]))\n",
    "        b = np.dot(1 - y[i,:], np.log(1-h[i,:]))\n",
    "        cost += np.sum(a - b)\n",
    "        \n",
    "    cost = cost/m\n",
    "    cost += reg/(2*m) * (np.sum(theta1[:, 1:]**2) + np.sum(theta2[:, 1:]**2))\n",
    "        \n",
    "    # Back-propagation\n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    for t in range(m):\n",
    "        a1t = a1[t,:]\n",
    "        a2t = a2[t,:]\n",
    "        ht = h[t,:]\n",
    "        yt = y[t]\n",
    "        \n",
    "        d3 = ht - yt\n",
    "        d2 = np.dot(theta2.T, d3) * (a2t * (1 - a2t))\n",
    "        \n",
    "        delta1 += np.dot(d2[1:, np.newaxis], a1t[np.newaxis, :])\n",
    "        delta2 += np.dot(d3[:, np.newaxis], a2t[np.newaxis, :])\n",
    "        \n",
    "    # Calculo del gradiente\n",
    "    D1 = delta1 / m\n",
    "    D2 = delta2 / m\n",
    "    \n",
    "    gradient = np.concatenate((np.ravel(D1), np.ravel(D2)))\n",
    "    \n",
    "    return cost, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo uso de la función ```checkNNGradients``` comprobamos si hemos implementado bien el cálculo del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.27761168e-11,  1.89059467e-12,  7.89324509e-12,  6.95584909e-12,\n",
       "       -6.30465125e-11,  2.08456863e-12, -1.07556394e-11, -5.04682407e-11,\n",
       "       -9.07785513e-11,  7.04843475e-12, -3.98116679e-11, -1.22385352e-10,\n",
       "       -2.17855040e-11,  2.76547969e-12, -6.02735570e-12, -2.49761462e-11,\n",
       "        2.15736526e-11, -4.96176017e-13,  1.19978506e-11,  2.73879391e-11,\n",
       "        6.25964836e-11,  1.55131741e-11,  9.03210839e-12,  5.26763355e-12,\n",
       "        1.90088223e-11,  1.88441207e-11,  7.15513759e-11,  1.56080426e-11,\n",
       "        7.11190828e-12,  1.37491546e-11,  1.70987668e-11,  1.79336823e-11,\n",
       "        7.32915950e-11,  1.60134683e-11,  8.61832827e-12,  1.78091986e-11,\n",
       "        1.43913215e-11,  2.26750840e-11])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from checkNNGradients import checkNNGradients\n",
    "checkNNGradients(backprop, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente regularizado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, para terminar la implementación del algoritmo de retropropagación añadimos el coeficiente de regularización al calculo del gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg=0):\n",
    "    '''Funcion de back-propagation para red neuronal de 3 capas'''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Desplegamos los paramas_rn en la matrices Theta\n",
    "    theta1 = np.reshape(params_rn[:num_ocultas * (num_entradas + 1)],\n",
    "                       (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(params_rn[num_ocultas * (num_entradas + 1):],\n",
    "                       (num_etiquetas, (num_ocultas + 1)))\n",
    "    \n",
    "    # Aplicamos forward-propagation para calcular la salidas de cada capa\n",
    "    a1, z2, a2, z3, h = forward_propagation(X, theta1, theta2)\n",
    "    \n",
    "    # Calculo del coste\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        a = np.dot(-y[i,:], np.log(h[i,:]))\n",
    "        b = np.dot((1-y[i,:]), np.log(1-h[i,:]))\n",
    "        cost += np.sum(a - b)\n",
    "        \n",
    "    cost = cost/m\n",
    "    \n",
    "    # Regularizacion del coste\n",
    "    cost += reg/(2*m) * (np.sum(theta1[:, 1:]**2) + np.sum(theta2[:, 1:]**2))\n",
    "    \n",
    "    # Back-propagation\n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    for t in range(m):\n",
    "        a1t = a1[t,:]\n",
    "        a2t = a2[t,:]\n",
    "        ht = h[t,:]\n",
    "        yt = y[t]\n",
    "        \n",
    "        d3 = ht - yt\n",
    "        d2 = np.dot(theta2.T, d3) * (a2t * (1 - a2t))\n",
    "        \n",
    "        delta1 += np.dot(d2[1:, np.newaxis], a1t[np.newaxis, :])\n",
    "        delta2 += np.dot(d3[:, np.newaxis], a2t[np.newaxis, :])\n",
    "        \n",
    "    # Calculo del gradiente\n",
    "    D1 = delta1 / m\n",
    "    D2 = delta2 / m\n",
    "    \n",
    "    # Regularizacion del gradiente\n",
    "    D1[:, 1:] = D1[:, 1:] + (reg * theta1[:, 1:]) / m\n",
    "    D2[:, 1:] = D2[:, 1:] + (reg * theta2[:, 1:]) / m\n",
    "    \n",
    "    gradient = np.concatenate((np.ravel(D1), np.ravel(D2)))\n",
    "    \n",
    "    return cost, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a comprobar si la implementación es correcta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.27761168e-11,  7.32719441e-13,  8.82988127e-12,  7.53047624e-12,\n",
       "       -6.30465125e-11,  2.10970130e-12, -1.16537613e-11, -4.92537400e-11,\n",
       "       -9.07785513e-11,  5.59484403e-12, -3.90588950e-11, -1.22203025e-10,\n",
       "       -2.17855040e-11,  4.35645964e-12, -7.00919878e-12, -2.43030734e-11,\n",
       "        2.15736526e-11,  2.27623476e-13,  1.19978506e-11,  2.84505197e-11,\n",
       "        6.25964836e-11,  1.38673517e-11,  8.50600146e-12,  5.29278010e-12,\n",
       "        2.03311395e-11,  1.78381754e-11,  7.15513759e-11,  1.63749014e-11,\n",
       "        7.86468113e-12,  1.39315087e-11,  1.64833286e-11,  1.95246597e-11,\n",
       "        7.32915950e-11,  1.66865410e-11,  8.55090998e-12,  1.63125624e-11,\n",
       "        1.34624811e-11,  2.22044327e-11])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from checkNNGradients import checkNNGradients\n",
    "checkNNGradients(backprop, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez implementado el algoritmo de retropropagación al completo, entrenamos a la red neuronal usando la función ```minimize``` de ```scipy.optimize``` y estudiamos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, reg, iters):\n",
    "    num_entradas = X.shape[1]\n",
    "    num_ocultas = 25\n",
    "    num_etiquetas = 10\n",
    "\n",
    "    theta1 = pesosAleatorios(num_entradas, num_ocultas)\n",
    "    theta2 = pesosAleatorios(num_ocultas, num_etiquetas)\n",
    "    params = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "\n",
    "    fmin = opt.minimize(fun=backprop, x0=params, \n",
    "                 args=(num_entradas, num_ocultas, num_etiquetas, X, y, reg),\n",
    "                 method='TNC', jac=True, options={'maxiter' : iters})\n",
    "\n",
    "    theta1 = np.reshape(fmin.x[:num_ocultas * (num_entradas + 1)],\n",
    "                       (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(fmin.x[num_ocultas * (num_entradas + 1):],\n",
    "                       (num_etiquetas, (num_ocultas + 1)))\n",
    "\n",
    "    a1, z2, a2, z2, h = forward_propagation(X, theta1, theta2)\n",
    "\n",
    "    predictions = np.argmax(h, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de fallos: 307\n",
      "Numero de aciertos: 4693\n",
      "\n",
      "Porcentaje de aciertos:  93.86\n"
     ]
    }
   ],
   "source": [
    "predictions = train(X, y_onehot, reg=1, iters=70)\n",
    "\n",
    "fallos =  np.where([predictions != y])[1]\n",
    "print('Numero de fallos:', len(fallos))\n",
    "\n",
    "aciertos = np.where([predictions == y])[1]\n",
    "print('Numero de aciertos:', len(aciertos))\n",
    "\n",
    "accuracy = 100 * np.mean(predictions == y)\n",
    "print(\"\\nPorcentaje de aciertos: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Con un parametro de regularización a 1 y un máximo de 70 iteraciones obtenemos una pracisión bastante buena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos a variar la regularizción y/o el número de iteraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de fallos: 4500\n",
      "Numero de aciertos: 500\n",
      "\n",
      "Porcentaje de aciertos:  10.0\n"
     ]
    }
   ],
   "source": [
    "predictions = train(X, y_onehot, reg=1, iters=10)\n",
    "\n",
    "fallos =  np.where([predictions != y])[1]\n",
    "print('Numero de fallos:', len(fallos))\n",
    "\n",
    "aciertos = np.where([predictions == y])[1]\n",
    "print('Numero de aciertos:', len(aciertos))\n",
    "\n",
    "accuracy = 100 * np.mean(predictions == y)\n",
    "print(\"\\nPorcentaje de aciertos: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de fallos: 1808\n",
      "Numero de aciertos: 3192\n",
      "\n",
      "Porcentaje de aciertos:  63.839999999999996\n"
     ]
    }
   ],
   "source": [
    "predictions = train(X, y_onehot, reg=0, iters=10)\n",
    "\n",
    "fallos =  np.where([predictions != y])[1]\n",
    "print('Numero de fallos:', len(fallos))\n",
    "\n",
    "aciertos = np.where([predictions == y])[1]\n",
    "print('Numero de aciertos:', len(aciertos))\n",
    "\n",
    "accuracy = 100 * np.mean(predictions == y)\n",
    "print(\"\\nPorcentaje de aciertos: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vemos que con pocas iteraciones no conseguimos una buena precisión. Esto se debe a que al aplicar pocas veces la retropropagación minimizamos muy poco el error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de fallos: 532\n",
      "Numero de aciertos: 4468\n",
      "\n",
      "Porcentaje de aciertos:  89.36\n"
     ]
    }
   ],
   "source": [
    "predictions = train(X, y_onehot, reg=20, iters=70)\n",
    "\n",
    "fallos =  np.where([predictions != y])[1]\n",
    "print('Numero de fallos:', len(fallos))\n",
    "\n",
    "aciertos = np.where([predictions == y])[1]\n",
    "print('Numero de aciertos:', len(aciertos))\n",
    "\n",
    "accuracy = 100 * np.mean(predictions == y)\n",
    "print(\"\\nPorcentaje de aciertos: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Si aumentamos la regularización obtenemos peor precisión, probablemente porque la red esté sobreaprendiendo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
